# Strategy 2: Code-Aware Analyzer Implementation Plan

**Feature**: `planloop suggest` - Proactively discover and suggest tasks from codebase analysis

**Timeline**: 5-7 days  
**Status**: Planning Complete - Ready for Implementation

---

## Overview

The `planloop suggest` command analyzes the codebase and current plan to proactively discover gaps, technical debt, and improvement opportunities. It generates contextualized task suggestions that AI agents can immediately understand and implement.

### Key Benefits
- **Proactive Discovery**: Finds work before users know to ask
- **Context-Rich**: Every suggestion includes file paths, patterns, and implementation notes
- **Agent-Friendly**: Suggestions include enough context for autonomous implementation
- **Non-Blocking**: Runs as separate command, doesn't interfere with existing workflow

---

## Architecture

### Component Structure
```
src/planloop/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ suggest.py              # Core suggestion engine
â”‚   â”œâ”€â”€ context_builder.py      # Codebase context analysis
â”‚   â””â”€â”€ llm_client.py           # LLM abstraction layer
â”œâ”€â”€ cli.py                      # Add suggest command
â””â”€â”€ config.py                   # Add suggest config schema

tests/
â”œâ”€â”€ test_suggest.py             # Suggestion engine tests
â”œâ”€â”€ test_context_builder.py    # Context builder tests
â””â”€â”€ test_llm_client.py          # LLM client tests
```

### Data Flow
```
User runs: planloop suggest
       â†“
ContextBuilder analyzes codebase
  - Project structure
  - Current plan tasks
  - Recent git changes
  - Test coverage gaps
  - TODO/FIXME comments
       â†“
LLMClient sends to AI
  - Context + analysis prompt
  - Structured output schema
       â†“
Suggestion engine validates
  - Check for duplicates
  - Verify file paths exist
  - Validate dependencies
       â†“
Present to user
  - Interactive approval
  - Edit before adding
  - Save to plan via update
```

---

## Implementation Tasks

### Phase 1: Foundation (Days 1-2)

#### Task 1.1: LLM Client Abstraction
**File**: `src/planloop/core/llm_client.py`

**Requirements**:
- Abstract interface for multiple LLM providers (OpenAI, Anthropic, local models)
- Configuration via `~/.planloop/config.yml` or env vars
- JSON mode for structured output
- Error handling and retries
- Token usage tracking

**Schema**:
```python
class LLMConfig(BaseModel):
    provider: Literal["openai", "anthropic", "ollama"]
    model: str
    api_key: str | None = None
    base_url: str | None = None
    temperature: float = 0.7
    max_tokens: int = 4000

class LLMClient:
    def __init__(self, config: LLMConfig): ...
    def generate(self, prompt: str, schema: dict | None = None) -> str: ...
    def generate_json(self, prompt: str, schema: dict) -> dict: ...
```

**Tests**:
- Mock provider responses
- Schema validation
- Error handling
- Configuration loading

---

#### Task 1.2: Context Builder Core
**File**: `src/planloop/core/context_builder.py`

**Requirements**:
- Build tiered context (project/module/file levels)
- Analyze file structure and imports
- Parse current plan tasks
- Extract TODO/FIXME/NOTE comments
- Git history analysis (recent changes, hotspots)
- Configurable depth limits

**Schema**:
```python
class CodebaseContext(BaseModel):
    project_root: Path
    structure: dict[str, Any]  # File tree
    current_tasks: list[Task]
    recent_changes: list[str]  # Changed files
    todos: list[TodoComment]
    language_stats: dict[str, int]
    
class TodoComment(BaseModel):
    file: str
    line: int
    type: Literal["TODO", "FIXME", "NOTE", "HACK"]
    text: str
    
class ContextBuilder:
    def __init__(self, session_path: Path): ...
    def build(self, depth: Literal["shallow", "medium", "deep"]) -> CodebaseContext: ...
```

**Tests**:
- File structure parsing
- TODO extraction
- Git history parsing
- Context depth filtering

---

#### Task 1.3: Configuration Schema
**File**: `src/planloop/config.py` (extend existing)

**Requirements**:
- Add `suggest` section to config
- LLM provider settings
- Context depth defaults
- Filter rules (ignore patterns)

**Schema**:
```yaml
suggest:
  llm:
    provider: openai  # or anthropic, ollama
    model: gpt-4
    api_key_env: OPENAI_API_KEY
    temperature: 0.7
    max_tokens: 4000
  context:
    depth: medium  # shallow, medium, deep
    include_git_history: true
    max_recent_commits: 10
    include_todos: true
  filters:
    ignore_patterns:
      - "*.pyc"
      - "__pycache__"
      - ".git"
      - "node_modules"
    focus_paths: []  # Empty = all paths
```

**Tests**:
- Schema validation
- Environment variable override
- Default values

---

### Phase 2: Core Suggestion Engine (Days 3-4)

#### Task 2.1: Suggestion Engine
**File**: `src/planloop/core/suggest.py`

**Requirements**:
- Orchestrate context building + LLM call
- Prompt engineering for task suggestions
- Parse and validate LLM output
- Check for duplicate tasks
- Dependency validation
- Priority scoring

**Core Logic**:
```python
class SuggestionEngine:
    def __init__(self, session: SessionState, config: SuggestConfig): ...
    
    def generate_suggestions(
        self, 
        context_depth: str = "medium",
        focus_area: str | None = None
    ) -> list[TaskSuggestion]: ...
    
    def _build_prompt(self, context: CodebaseContext) -> str: ...
    def _validate_suggestion(self, suggestion: TaskSuggestion) -> bool: ...
    def _check_duplicates(self, suggestion: TaskSuggestion) -> bool: ...

class TaskSuggestion(BaseModel):
    title: str
    type: TaskType
    priority: Literal["low", "medium", "high"]
    rationale: str
    implementation_notes: str
    affected_files: list[str]
    depends_on: list[int] = []
```

**Prompt Template**:
```markdown
# Role
You are a technical project manager analyzing a codebase to suggest concrete, actionable tasks.

# Context
Project: {project_name}
Current Plan: {current_tasks}
File Structure: {structure}
Recent Changes: {git_history}
TODOs: {todo_comments}

# Your Job
Analyze the codebase and suggest 3-5 high-impact tasks:
1. Find gaps (missing tests, docs, error handling)
2. Identify technical debt (TODOs, hacks, deprecated patterns)
3. Spot improvement opportunities (performance, security, UX)

# Rules
- Be specific (name files, functions, patterns)
- Prioritize based on impact and risk
- Check current plan for duplicates
- Suggest dependencies between tasks

# Output Format
Return JSON array matching TaskSuggestion schema:
{schema}
```

**Tests**:
- Prompt generation
- LLM output parsing
- Duplicate detection
- Priority scoring
- Validation logic

---

#### Task 2.2: CLI Integration
**File**: `src/planloop/cli.py`

**Requirements**:
- Add `@app.command()` for `suggest`
- Interactive approval flow
- Edit suggestions before adding
- Batch add to plan via update payload
- Dry-run mode

**Command Signature**:
```python
@app.command()
def suggest(
    session: str | None = typer.Option(None, help="Session ID"),
    depth: Literal["shallow", "medium", "deep"] = typer.Option("medium", help="Context depth"),
    focus: str | None = typer.Option(None, help="Focus area (e.g., 'src/auth')"),
    auto_approve: bool = typer.Option(False, help="Skip interactive approval"),
    dry_run: bool = typer.Option(False, help="Preview without adding"),
    limit: int = typer.Option(5, help="Max suggestions"),
) -> None:
    """Analyze codebase and suggest tasks."""
```

**Interactive Flow**:
```
$ planloop suggest

ğŸ” Analyzing codebase...
âœ“ Found 5 suggestions

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Suggestion 1/5 [HIGH PRIORITY]

Title: Add error handling to API endpoints
Type: bugfix
Rationale: 12 endpoints missing try/catch, could leak exceptions
Files: src/api/routes.py, src/api/handlers.py
Notes: Pattern exists in src/auth/routes.py - replicate

[a]dd [e]dit [s]kip [q]uit? a

Suggestion 2/5 [MEDIUM PRIORITY]
...

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Added 3 tasks to plan
Skipped: 2
```

**Tests**:
- Command execution
- Interactive flow (mocked input)
- Dry-run mode
- Batch update payload generation

---

### Phase 3: Discoverability & Documentation (Day 5)

#### Task 3.1: Agent Discoverability
**File**: `docs/agents.md` (update)

**Requirements**:
- Add `suggest` to key commands section
- Document when agents should use it
- Provide example workflows

**Content**:
```markdown
## Key commands
- `planloop suggest` â†’ analyze codebase and discover new tasks proactively.
  Use this when:
  - Current plan is empty or near completion
  - You want to find technical debt or gaps
  - User asks "what should I work on next?"
  - Need context-aware task ideas

Example workflow:
1. Run `planloop status` - no tasks left
2. Run `planloop suggest` - discover 5 new tasks
3. Approve relevant tasks
4. Run `planloop status` - pick up first task
```

---

#### Task 3.2: Status Output Integration
**File**: `src/planloop/core/state.py` (extend)

**Requirements**:
- Add hint in `agent_instructions` when no tasks exist
- Suggest running `planloop suggest`

**Logic**:
```python
# In compute_now() when reason="completed"
if not state.tasks:
    instructions = "No tasks defined. Run 'planloop suggest' to discover work."
elif all(t.status == TaskStatus.DONE for t in state.tasks):
    instructions = "All tasks complete! Run 'planloop suggest' to find more work, or add final_summary."
```

**Tests**:
- Verify hint appears in status output
- Check JSON structure

---

#### Task 3.3: Help Text & README
**Files**: 
- `src/planloop/cli.py` (command help)
- `README.md` (feature documentation)

**Requirements**:
- Clear help text for `planloop suggest --help`
- Add to README command table
- Quick start example

**README Section**:
```markdown
### Discovering Tasks with AI

planloop can analyze your codebase and suggest tasks automatically:

```bash
# Analyze codebase and get suggestions
planloop suggest

# Focus on specific area
planloop suggest --focus src/auth

# Deep analysis (slower, more thorough)
planloop suggest --depth deep

# Preview without adding
planloop suggest --dry-run
```

The AI looks for:
- Missing tests and documentation
- TODO/FIXME comments to address
- Technical debt patterns
- Improvement opportunities
```

---

### Phase 4: Testing & Polish (Days 6-7)

#### Task 4.1: Integration Tests
**File**: `tests/test_suggest_integration.py`

**Requirements**:
- End-to-end test with real file structure
- Mock LLM responses
- Verify full workflow
- Test error cases

**Scenarios**:
- Empty plan â†’ suggests tasks
- Existing tasks â†’ no duplicates
- Invalid LLM output â†’ graceful handling
- Focus area â†’ filtered suggestions

---

#### Task 4.2: Performance Optimization
**File**: `src/planloop/core/context_builder.py`

**Requirements**:
- Cache file structure scans
- Lazy loading for deep context
- Parallel file processing
- Benchmark large codebases

**Targets**:
- <2s for shallow context
- <5s for medium context
- <15s for deep context

---

#### Task 4.3: Error Handling & Edge Cases
**Files**: All core modules

**Requirements**:
- Missing API keys â†’ clear error message
- No git repo â†’ graceful degradation
- Empty codebase â†’ handle elegantly
- LLM timeout â†’ retry logic
- Rate limits â†’ backoff strategy

---

## Configuration Example

**`~/.planloop/config.yml`**:
```yaml
suggest:
  llm:
    provider: openai
    model: gpt-4o-mini
    api_key_env: OPENAI_API_KEY
    temperature: 0.7
    max_tokens: 4000
  
  context:
    depth: medium
    include_git_history: true
    max_recent_commits: 10
    include_todos: true
    max_files_sample: 100
  
  filters:
    ignore_patterns:
      - "*.pyc"
      - "__pycache__"
      - ".git"
      - "node_modules"
      - "*.egg-info"
    focus_paths: []  # Empty = analyze all

  suggestions:
    max_count: 5
    min_priority: low
    auto_approve: false
```

---

## Testing Strategy

### Unit Tests (pytest)
- `test_llm_client.py` - Provider abstraction, mocking
- `test_context_builder.py` - Context extraction, parsing
- `test_suggest.py` - Suggestion generation, validation

### Integration Tests
- `test_suggest_integration.py` - Full workflow
- `test_cli_suggest.py` - Command execution

### Manual Testing Checklist
- [ ] Run on planloop itself
- [ ] Test with OpenAI
- [ ] Test with Anthropic (if configured)
- [ ] Test on empty project
- [ ] Test on large project (>1000 files)
- [ ] Test with no git repo
- [ ] Test with no API key
- [ ] Verify agent discoverability

---

## Success Criteria

### Functional
- âœ… Generates 3-5 relevant suggestions
- âœ… No duplicate tasks
- âœ… Valid file paths and dependencies
- âœ… Interactive approval works
- âœ… Integrates with existing update flow

### Performance
- âœ… <5s for medium context
- âœ… Handles 1000+ file codebases
- âœ… Graceful degradation on errors

### Discoverability
- âœ… Appears in `planloop status` hints
- âœ… Documented in `docs/agents.md`
- âœ… Clear help text
- âœ… AI agents use it autonomously

### Quality
- âœ… 90%+ test coverage
- âœ… All tests passing
- âœ… Linting clean
- âœ… Type hints complete

---

## Future Enhancements (Post-MVP)

### v2 Features
- **Embeddings-based search**: Use vector DB for semantic code search
- **Learning from feedback**: Track accepted/rejected suggestions
- **Batch workflows**: `planloop suggest --weekly` for regular audits
- **Custom analyzers**: Plugin system for domain-specific patterns
- **Diff suggestions**: "This PR introduces X, suggest follow-up tasks"

### Advanced Analysis
- **Test coverage gaps**: Integrate with pytest-cov
- **Security patterns**: Static analysis (bandit, semgrep)
- **Performance profiling**: Suggest optimization tasks
- **Dependency updates**: Detect outdated packages

---

## Risk Mitigation

### Technical Risks
- **LLM hallucination**: Validate all file paths and dependencies
- **Cost concerns**: Default to smaller models, add token budgets
- **API rate limits**: Implement exponential backoff, caching

### UX Risks
- **Suggestion fatigue**: Limit to 5 suggestions, prioritize well
- **False positives**: Make approval easy, allow editing
- **Noise**: Learn from rejections (future enhancement)

### Integration Risks
- **Breaking changes**: Use feature flag initially
- **Config complexity**: Provide sensible defaults
- **Migration**: Graceful fallback if suggest unavailable

---

## Timeline Summary

- **Day 1**: Task 1.1 (LLM Client) + Task 1.3 (Config)
- **Day 2**: Task 1.2 (Context Builder)
- **Day 3**: Task 2.1 (Suggestion Engine)
- **Day 4**: Task 2.2 (CLI Integration)
- **Day 5**: Task 3.1-3.3 (Discoverability)
- **Day 6**: Task 4.1-4.2 (Integration tests, optimization)
- **Day 7**: Task 4.3 (Polish, manual testing, documentation)

**Total**: 7 days with built-in buffer for iteration

---

## Next Steps

1. Review this plan and adjust based on feedback
2. Add tasks to `docs/plan.md` backlog
3. Start with Task 1.1 (LLM Client) - most foundational
4. Practice TDD: write tests first, implement, refactor
5. Commit each task separately for clean history
