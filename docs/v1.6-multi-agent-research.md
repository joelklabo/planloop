# planloop v1.6 – Multi-Agent Locking Research

## Motivation
Milestone 13 uncovered the need for stronger coordination when multiple agents
share a PLAN.md/state.json pair. The current `.lock` implementation is a simple
binary mutex; agents that find the lock held spin until it is released, which:

- Gives no fairness guarantees (one agent could starve others by repeatedly
  reacquiring the lock).
- Offers no queue visibility so humans cannot tell who is waiting.
- Lacks telemetry (e.g., wait durations, deadlock patterns) needed to tune the
  workflow.

v1.6 should introduce a “lock queue” concept so agents can coordinate without
human babysitting and provide meaningful status output.

## Goals
1. **Fairness:** Ensure agents that request the lock are served in FIFO order
   and that humans can observe the queue.
2. **Transparency:** Surface the wait queue and estimator in `planloop status`
   (JSON + textual hints) so agents can decide whether to keep waiting or pivot.
3. **Extensibility:** Keep the design open for future features such as timeouts,
   lock lease extensions, and eventual multi-session orchestration.

## Non-Goals
- Implementing multi-session distributed locks (out of scope for v1.6).
- Changing the on-disk state format beyond adding metadata files for queueing.
- Handling cross-machine coordination; focus on a single PLANLOOP_HOME.

## Proposed Design
### Filesystem primitives
- Introduce `sessions/<id>/.lock_queue/` containing per-request JSON files.
  Each request file would include `agent_id`, `requested_at`, `timeout_ms`, and
  `operation` fields.
- Maintain a `queue.json` index to avoid scanning the directory frequently.
- Lock acquisition flow:
  1. Agent writes a queue entry (`.lock_queue/<uuid>.json`).
  2. Agents poll the queue; the one whose entry is at the head proceeds to
     acquire the actual `.lock` file (backward compatibility maintained).
  3. When finished, agent removes its queue entry and `.lock_info` file.
- If an agent crashes without cleaning up, a watchdog (on status calls) prunes
  stale queue entries using `requested_at + timeout_ms`.

### Status output
- Extend `planloop status` payload with:
  ```json
  "lock_queue": {
    "pending": [
      {"agent": "pid:123", "operation": "update", "requested_at": "..."},
      ...
    ],
    "position": 2
  }
  ```
- Agents use `position` to decide whether to wait or escalate.

### Deadlock integration
- `deadlock` tracker increments when the same agent repeatedly appears at the
  head without acquiring the lock; after N attempts, raise a `system/queue_stall`
  signal suggesting manual intervention.

## Telemetry & Metrics
- Append queue events to `logs/planloop.log` so humans can audit contention.
- Expose aggregated stats through `planloop debug --logs` (e.g., longest wait).

## Work Breakdown (suggested tasks)
1. **P3.1 – Queue metadata + status surfacing**
   - Implement queue directory/index, update lock acquisition to register and
     prune entries, and extend status output.
2. **P3.2 – Deadlock/timeout handling**
   - Integrate with existing `DeadlockTracker`, emit new `system/queue_stall`
     signals, and add tests for stale queue entries.
3. **P3.3 – Docs & prompt updates**
   - Teach AGENTS.md/README/prompt handshake how to interpret queue data and
     encourage agents to yield when they are not first in line.

## Open Questions
- Should queue entries include human-friendly identifiers (agent name) beyond
  PID? Possibly use CLI flags/env to pass `PLANLOOP_AGENT_NAME`.
- How to handle agents that abandon queue entries (e.g., due to network issues)?
  Options include TTL-based pruning or explicit `planloop queue leave` command.
- Do we need to persist queue history for analytics? (Maybe optional.)

## Next Steps
- Validate this design with stakeholders (human operators + agent authors).
- Once agreed, translate P3.1–P3.3 into plan tasks for v1.6 implementation.
